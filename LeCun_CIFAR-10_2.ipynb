{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_img(image):\n",
    "    return_img = np.zeros_like(image, dtype = float)\n",
    "    \n",
    "    r_img = image[:,0:1024]\n",
    "    r_avg = np.average(r_img)\n",
    "    r_std = np.std(r_img)\n",
    "    r_img = (r_img - r_avg)/r_std\n",
    "    return_img[:,0:1024] = r_img\n",
    "\n",
    "    g_img = image[:,1024:2048]\n",
    "    g_avg = np.average(g_img)\n",
    "    g_std = np.std(g_img)\n",
    "    g_img = (g_img - g_avg)/g_std\n",
    "    return_img[:,1024:2048] = g_img\n",
    "\n",
    "    b_img = image[:,2048:]\n",
    "    b_avg = np.average(b_img)\n",
    "    b_std = np.std(b_img)\n",
    "    b_img = (b_img - b_avg)/b_std\n",
    "    return_img[:,2048:] = b_img\n",
    "    \n",
    "    #return ((image/255.)-0.5)\n",
    "    return return_img\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "def unpickle_all(path):\n",
    "    train_images = np.zeros((50000,32,32,3))\n",
    "    train_stack = np.zeros((50000,3072))\n",
    "    test_images = np.zeros((10000,32,32,3))\n",
    "    train_labels = np.zeros((50000))\n",
    "    test_labels = np.zeros((10000))\n",
    "    for i in range(1,6):\n",
    "        j = i-1\n",
    "        cifar10 = unpickle(path+'/data_batch_{}'.format(i))\n",
    "        train_labels[j*10000:(j+1)*10000] = cifar10['labels']\n",
    "        #data = scale_img(cifar10['data'])\n",
    "        #train_images[j*10000:(j+1)*10000,:,:,:] = np.swapaxes(np.reshape(data,(10000,32,32,3), order='F'),axis1=1,axis2=2)\n",
    "        train_stack[j*10000:(j+1)*10000,:] = cifar10['data']\n",
    "    \n",
    "    train_stack = scale_img(train_stack)\n",
    "    train_images = np.swapaxes(np.reshape(train_stack,(50000,32,32,3), order='F'),axis1=1,axis2=2)\n",
    "    \n",
    "    cifar10 = unpickle(path + '/test_batch')\n",
    "    test_labels = cifar10['labels']\n",
    "    data = scale_img(cifar10['data'])\n",
    "    test_images = np.swapaxes(np.reshape(data,(10000,32,32,3), order='F'),axis1=1,axis2=2)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, test_images, test_labels = unpickle_all('./storage/CIFAR-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "momentum = 0.9\n",
    "lambd = 0.0005\n",
    "epoch = 200\n",
    "batch_size = 128\n",
    "display_step = 2\n",
    "total_batch = int(images.shape[0] / batch_size)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "boundaries = [100*total_batch-1, 150*total_batch]\n",
    "values = [0.001, 0.0005, 0.0001]\n",
    "LR = tf.train.piecewise_constant(global_step, boundaries, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float32', [None,32,32,3], name = \"X\")\n",
    "y = tf.placeholder('int64', [None,], name = \"Y\")\n",
    "\n",
    "#OUT_CL_1 : n x 14 x 14 x 6\n",
    "#OUT_CL_2 : n x 5 x 5 x 16 = n x 400\n",
    "#OUT_DENSE_1 : n x 120\n",
    "#OUT_DENSE_2 : n x 84\n",
    "\n",
    "W_CL_1 = tf.Variable(np.random.randn(5, 5, 3, 6)/25, dtype=\"float32\", name = \"W_CL_1\")\n",
    "b_CL_1 = tf.Variable(np.random.randn(1, 1, 1, 6), dtype=\"float32\", name = \"b_CL_1\")\n",
    "W_CL_2 = tf.Variable(np.random.randn(5, 5, 6, 16)/25, dtype=\"float32\", name = \"W_CL_2\")\n",
    "b_CL_2 = tf.Variable(np.random.randn(1, 1, 1, 16), dtype=\"float32\", name = \"b_CL_2\")\n",
    "\n",
    "REG_W_CL_1 = tf.square(tf.norm(W_CL_1))\n",
    "REG_b_CL_1 = tf.square(tf.norm(b_CL_1))\n",
    "REG_W_CL_2 = tf.square(tf.norm(W_CL_2))\n",
    "REG_b_CL_2 = tf.square(tf.norm(b_CL_2))\n",
    "\n",
    "REGUL = lambd*(REG_W_CL_1 + REG_b_CL_1 + REG_W_CL_2 + REG_b_CL_2)/(2 * batch_size)\n",
    "\n",
    "with tf.name_scope(\"CL1\"):\n",
    "    CL_1 = tf.nn.conv2d(x,W_CL_1,strides = [1,1,1,1], padding=\"VALID\", name = \"CL_1\")\n",
    "    RELU_1 = tf.nn.relu(tf.add(CL_1,b_CL_1), name = \"RELU_1\")\n",
    "    MAXPOOL_1 = tf.nn.max_pool(RELU_1,ksize=(1,2,2,1),strides=(1,2,2,1),padding=\"VALID\", name = \"MAXPOOL_1\")\n",
    "\n",
    "with tf.name_scope(\"CL2\"):\n",
    "    CL_2 = tf.nn.conv2d(MAXPOOL_1,W_CL_2,strides = [1,1,1,1], padding=\"SAME\", name = \"CL_2\")\n",
    "    RELU_2 = tf.nn.relu(tf.add(CL_2,b_CL_2), name = \"RELU_2\")\n",
    "    MAXPOOL_2 = tf.nn.max_pool(RELU_2,ksize=(1,2,2,1),strides=(1,2,2,1),padding=\"VALID\", name = \"MAXPOOL_2\")\n",
    "    FLAT_2 = tf.layers.flatten(MAXPOOL_2, name = \"FLAT_2\")\n",
    "\n",
    "with tf.name_scope(\"DENSE_1\"):\n",
    "    DENSE_1 = tf.contrib.layers.fully_connected(FLAT_2,120,weights_regularizer= tf.contrib.layers.l2_regularizer(lambd), biases_regularizer =tf.contrib.layers.l2_regularizer(lambd))\n",
    "\n",
    "with tf.name_scope(\"DENSE_2\"):\n",
    "    DENSE_2 = tf.contrib.layers.fully_connected(DENSE_1,84,weights_regularizer= tf.contrib.layers.l2_regularizer(lambd), biases_regularizer =tf.contrib.layers.l2_regularizer(lambd))\n",
    "\n",
    "with tf.name_scope(\"LOGITS_1\"):\n",
    "    LOGITS_1 = tf.contrib.layers.fully_connected(DENSE_2,10, activation_fn = None,weights_regularizer= tf.contrib.layers.l2_regularizer(lambd), biases_regularizer =tf.contrib.layers.l2_regularizer(lambd))\n",
    "\n",
    "with tf.name_scope(\"PREDICTIONS\"):\n",
    "    y_hat = tf.nn.softmax(LOGITS_1, name = \"y_hat\")\n",
    "    predictions = tf.argmax(y_hat,1)\n",
    "\n",
    "with tf.name_scope(\"LOSS\"):\n",
    "    loss = tf.add(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(y, depth=10),logits=LOGITS_1)),REGUL)\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(y, depth=10),logits=LOGITS_1))\n",
    "    tf.summary.scalar(\"cost_function\", loss)\n",
    "\n",
    "with tf.name_scope(\"ACCURACY\"):\n",
    "    compare = tf.equal(predictions, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(compare, tf.float32))\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "with tf.name_scope(\"OPTIMIZER\"):\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate = LR, momentum = momentum, name=\"opt\").minimize(loss, global_step = global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "summ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0  Cost= 0.031601909216111286  Train accuracy= 21.10176282051285%  Test accuracy= 26.930001378059387%\n",
      "Iteration: 2  Cost= 0.013751865675052016  Train accuracy= 35.751201923077005%  Test accuracy= 36.86000108718872%\n",
      "Iteration: 4  Cost= 0.012353908366117723  Train accuracy= 42.70633012820516%  Test accuracy= 40.529999136924744%\n",
      "Iteration: 6  Cost= 0.011462590941347372  Train accuracy= 47.12339743589747%  Test accuracy= 45.03999948501587%\n",
      "Iteration: 8  Cost= 0.01078812735489545  Train accuracy= 50.45472756410261%  Test accuracy= 47.17000126838684%\n",
      "Iteration: 10  Cost= 0.01029966220689508  Train accuracy= 52.844551282051334%  Test accuracy= 48.69999885559082%\n",
      "Iteration: 12  Cost= 0.009896101167377757  Train accuracy= 54.73758012820519%  Test accuracy= 49.77000057697296%\n",
      "Iteration: 14  Cost= 0.009577940326804922  Train accuracy= 56.268028846153896%  Test accuracy= 50.41000247001648%\n",
      "Iteration: 16  Cost= 0.009316392481709136  Train accuracy= 57.4859775641026%  Test accuracy= 51.829999685287476%\n",
      "Iteration: 18  Cost= 0.009056485482515437  Train accuracy= 58.641826923076934%  Test accuracy= 52.0799994468689%\n",
      "Iteration: 20  Cost= 0.008803927741992549  Train accuracy= 59.66947115384617%  Test accuracy= 52.93999910354614%\n",
      "Iteration: 22  Cost= 0.008592290544691376  Train accuracy= 60.70713141025651%  Test accuracy= 53.140002489089966%\n",
      "Iteration: 24  Cost= 0.008349568372926653  Train accuracy= 61.91907051282054%  Test accuracy= 53.039997816085815%\n",
      "Iteration: 26  Cost= 0.008174954352375028  Train accuracy= 62.6822916666667%  Test accuracy= 54.24000024795532%\n",
      "Iteration: 28  Cost= 0.00796714290881004  Train accuracy= 63.597756410256466%  Test accuracy= 54.39000129699707%\n",
      "Iteration: 30  Cost= 0.007739559501314012  Train accuracy= 64.65144230769234%  Test accuracy= 54.32000160217285%\n",
      "Iteration: 32  Cost= 0.0075505197442208415  Train accuracy= 65.78125000000001%  Test accuracy= 54.579997062683105%\n",
      "Iteration: 34  Cost= 0.007356858886300755  Train accuracy= 66.80488782051283%  Test accuracy= 54.82000112533569%\n",
      "Iteration: 36  Cost= 0.007175619986194829  Train accuracy= 67.75040064102564%  Test accuracy= 54.989999532699585%\n",
      "Iteration: 38  Cost= 0.007028512719970863  Train accuracy= 68.41145833333337%  Test accuracy= 54.4700026512146%\n",
      "Iteration: 40  Cost= 0.006826025460106442  Train accuracy= 69.2447916666667%  Test accuracy= 54.4700026512146%\n",
      "Iteration: 42  Cost= 0.006686724682983302  Train accuracy= 69.86378205128206%  Test accuracy= 54.06000018119812%\n",
      "Iteration: 44  Cost= 0.006545212341902343  Train accuracy= 70.62299679487182%  Test accuracy= 54.06000018119812%\n",
      "Iteration: 46  Cost= 0.006452081941115927  Train accuracy= 71.13181089743595%  Test accuracy= 54.04999852180481%\n",
      "Iteration: 48  Cost= 0.0063175207099471315  Train accuracy= 71.70873397435895%  Test accuracy= 54.46000099182129%\n",
      "Iteration: 50  Cost= 0.0062367921671233105  Train accuracy= 72.01722756410255%  Test accuracy= 53.860002756118774%\n",
      "Iteration: 52  Cost= 0.0061150161883769885  Train accuracy= 72.55008012820511%  Test accuracy= 54.500001668930054%\n",
      "Iteration: 54  Cost= 0.005970216684568767  Train accuracy= 73.2532051282052%  Test accuracy= 53.97999882698059%\n",
      "Iteration: 56  Cost= 0.005913340678820631  Train accuracy= 73.5156249999999%  Test accuracy= 54.55999970436096%\n",
      "Iteration: 58  Cost= 0.005775924237110674  Train accuracy= 74.24479166666666%  Test accuracy= 54.12999987602234%\n",
      "Iteration: 60  Cost= 0.005664874189413893  Train accuracy= 74.67548076923077%  Test accuracy= 53.78999710083008%\n",
      "Iteration: 62  Cost= 0.0056161701016557925  Train accuracy= 74.90985576923073%  Test accuracy= 53.769999742507935%\n",
      "Iteration: 64  Cost= 0.005473891172844623  Train accuracy= 75.58092948717949%  Test accuracy= 53.53999733924866%\n",
      "Iteration: 66  Cost= 0.0053772950347942845  Train accuracy= 76.02564102564105%  Test accuracy= 53.07000279426575%\n",
      "Iteration: 68  Cost= 0.0052556148545147875  Train accuracy= 76.5925480769231%  Test accuracy= 52.920001745224%\n",
      "Iteration: 70  Cost= 0.00519514910948391  Train accuracy= 76.76482371794869%  Test accuracy= 52.66000032424927%\n",
      "Iteration: 72  Cost= 0.0051593399069343685  Train accuracy= 77.19150641025638%  Test accuracy= 52.730000019073486%\n",
      "Iteration: 74  Cost= 0.005067695639669324  Train accuracy= 77.37379807692305%  Test accuracy= 52.30000019073486%\n",
      "Iteration: 76  Cost= 0.0049725877664362405  Train accuracy= 77.92267628205134%  Test accuracy= 52.49999761581421%\n",
      "Iteration: 78  Cost= 0.004886958096176386  Train accuracy= 78.2612179487179%  Test accuracy= 52.480000257492065%\n",
      "Iteration: 80  Cost= 0.004833526111160139  Train accuracy= 78.4735576923077%  Test accuracy= 51.95000171661377%\n",
      "Iteration: 82  Cost= 0.00481330841982689  Train accuracy= 78.74599358974356%  Test accuracy= 52.0799994468689%\n",
      "Iteration: 84  Cost= 0.004681597861389702  Train accuracy= 79.33092948717956%  Test accuracy= 51.94000005722046%\n",
      "Iteration: 86  Cost= 0.004581368913372547  Train accuracy= 79.75761217948721%  Test accuracy= 51.96999907493591%\n",
      "Iteration: 88  Cost= 0.004578938602040021  Train accuracy= 79.66145833333331%  Test accuracy= 52.20999717712402%\n",
      "Iteration: 90  Cost= 0.0046452900126146566  Train accuracy= 79.43910256410263%  Test accuracy= 52.38000154495239%\n",
      "Iteration: 92  Cost= 0.004434055227260944  Train accuracy= 80.37459935897436%  Test accuracy= 52.230000495910645%\n",
      "Iteration: 94  Cost= 0.004422849220916249  Train accuracy= 80.53084935897441%  Test accuracy= 52.0799994468689%\n",
      "Iteration: 96  Cost= 0.004371532105291504  Train accuracy= 80.67508012820515%  Test accuracy= 51.829999685287476%\n",
      "Iteration: 98  Cost= 0.004330001222208524  Train accuracy= 80.72716346153838%  Test accuracy= 51.13999843597412%\n",
      "Iteration: 100  Cost= 0.004219381808518217  Train accuracy= 81.35817307692314%  Test accuracy= 51.749998331069946%\n",
      "Iteration: 102  Cost= 0.004328215801013776  Train accuracy= 80.90945512820521%  Test accuracy= 51.52999758720398%\n",
      "Iteration: 104  Cost= 0.00418828318122392  Train accuracy= 81.50641025641025%  Test accuracy= 51.48000121116638%\n",
      "Iteration: 106  Cost= 0.004166959192699345  Train accuracy= 81.66866987179485%  Test accuracy= 51.77000164985657%\n",
      "Iteration: 108  Cost= 0.004072217784153343  Train accuracy= 82.02724358974362%  Test accuracy= 51.749998331069946%\n",
      "Iteration: 110  Cost= 0.004032358946875697  Train accuracy= 82.23758012820514%  Test accuracy= 51.3700008392334%\n",
      "Iteration: 112  Cost= 0.004064376724477951  Train accuracy= 81.98717948717949%  Test accuracy= 51.9599974155426%\n",
      "Iteration: 114  Cost= 0.003933436998452702  Train accuracy= 82.59615384615377%  Test accuracy= 51.56000256538391%\n",
      "Iteration: 116  Cost= 0.003975685687664035  Train accuracy= 82.40184294871796%  Test accuracy= 52.160000801086426%\n",
      "Iteration: 118  Cost= 0.003837295407906937  Train accuracy= 82.9967948717949%  Test accuracy= 51.63999795913696%\n",
      "Iteration: 120  Cost= 0.003871615437002712  Train accuracy= 83.07491987179499%  Test accuracy= 51.31999850273132%\n",
      "Iteration: 122  Cost= 0.003855753295087756  Train accuracy= 82.98677884615378%  Test accuracy= 51.46999955177307%\n",
      "Iteration: 124  Cost= 0.003732426046656489  Train accuracy= 83.47956730769228%  Test accuracy= 51.80000066757202%\n",
      "Iteration: 126  Cost= 0.0037563847145065635  Train accuracy= 83.45152243589742%  Test accuracy= 50.91000199317932%\n",
      "Iteration: 128  Cost= 0.003705520706716925  Train accuracy= 83.73597756410254%  Test accuracy= 51.38999819755554%\n",
      "Iteration: 130  Cost= 0.0037240930235920787  Train accuracy= 83.64783653846145%  Test accuracy= 51.41000151634216%\n",
      "Iteration: 132  Cost= 0.003636878732150086  Train accuracy= 83.9563301282051%  Test accuracy= 51.35999917984009%\n",
      "Iteration: 134  Cost= 0.0035861855811582734  Train accuracy= 84.228766025641%  Test accuracy= 51.23000144958496%\n",
      "Iteration: 136  Cost= 0.003669991982408253  Train accuracy= 83.95833333333333%  Test accuracy= 51.05999708175659%\n",
      "Iteration: 138  Cost= 0.003492839894412705  Train accuracy= 84.70552884615381%  Test accuracy= 51.190000772476196%\n",
      "Iteration: 140  Cost= 0.0036290916542594267  Train accuracy= 84.17868589743591%  Test accuracy= 50.919997692108154%\n",
      "Iteration: 142  Cost= 0.0034648468365701753  Train accuracy= 84.75160256410264%  Test accuracy= 51.15000009536743%\n",
      "Iteration: 144  Cost= 0.003368983798337959  Train accuracy= 85.21233974358977%  Test accuracy= 50.849997997283936%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 146  Cost= 0.0034980216869511323  Train accuracy= 84.59134615384617%  Test accuracy= 51.340001821517944%\n",
      "Iteration: 148  Cost= 0.003373323041850178  Train accuracy= 85.08012820512816%  Test accuracy= 51.30000114440918%\n",
      "Iteration: 150  Cost= 0.003389651645631649  Train accuracy= 85.13822115384616%  Test accuracy= 50.849997997283936%\n",
      "Iteration: 152  Cost= 0.003397538337426692  Train accuracy= 85.14222756410263%  Test accuracy= 50.7099986076355%\n",
      "Iteration: 154  Cost= 0.0033475369752313066  Train accuracy= 85.34855769230778%  Test accuracy= 50.62999725341797%\n",
      "Iteration: 156  Cost= 0.0033310298990004506  Train accuracy= 85.625%  Test accuracy= 50.679999589920044%\n",
      "Iteration: 158  Cost= 0.0032754861224347184  Train accuracy= 85.69511217948725%  Test accuracy= 51.15000009536743%\n",
      "Iteration: 160  Cost= 0.003274365069452101  Train accuracy= 85.75320512820514%  Test accuracy= 50.87000131607056%\n",
      "Iteration: 162  Cost= 0.0032130571040444255  Train accuracy= 85.89342948717953%  Test accuracy= 50.51000118255615%\n",
      "Iteration: 164  Cost= 0.0031235088764403293  Train accuracy= 86.3341346153846%  Test accuracy= 50.80999732017517%\n",
      "Iteration: 166  Cost= 0.0033026402189324667  Train accuracy= 85.55689102564095%  Test accuracy= 50.51000118255615%\n",
      "Iteration: 168  Cost= 0.003087742797707993  Train accuracy= 86.53445512820524%  Test accuracy= 50.76000094413757%\n",
      "Iteration: 170  Cost= 0.0031518264894540887  Train accuracy= 86.35416666666664%  Test accuracy= 50.72000026702881%\n",
      "Iteration: 172  Cost= 0.0031110696882630374  Train accuracy= 86.56850961538463%  Test accuracy= 50.34000277519226%\n",
      "Iteration: 174  Cost= 0.00303947394654059  Train accuracy= 86.77083333333324%  Test accuracy= 50.440001487731934%\n",
      "Iteration: 176  Cost= 0.0030078485214079824  Train accuracy= 86.90104166666663%  Test accuracy= 50.65000057220459%\n",
      "Iteration: 178  Cost= 0.0031629652465478735  Train accuracy= 86.31810897435888%  Test accuracy= 50.609999895095825%\n",
      "Iteration: 180  Cost= 0.003009013314910522  Train accuracy= 86.98317307692295%  Test accuracy= 51.010000705718994%\n",
      "Iteration: 182  Cost= 0.0029582385872251897  Train accuracy= 87.13741987179485%  Test accuracy= 50.01000165939331%\n",
      "Iteration: 184  Cost= 0.0029724630822714117  Train accuracy= 87.1754807692307%  Test accuracy= 49.70000088214874%\n",
      "Iteration: 186  Cost= 0.002961083951119621  Train accuracy= 87.1694711538462%  Test accuracy= 49.90000128746033%\n",
      "Iteration: 188  Cost= 0.002924602306507625  Train accuracy= 87.37379807692304%  Test accuracy= 50.26000142097473%\n",
      "Iteration: 190  Cost= 0.0029754828772125506  Train accuracy= 87.24358974358974%  Test accuracy= 50.4800021648407%\n",
      "Iteration: 192  Cost= 0.0029046369665779916  Train accuracy= 87.49799679487175%  Test accuracy= 49.82999861240387%\n",
      "Iteration: 194  Cost= 0.002874081945106481  Train accuracy= 87.50200320512826%  Test accuracy= 50.700002908706665%\n",
      "Iteration: 196  Cost= 0.002859765428225867  Train accuracy= 87.59415064102559%  Test accuracy= 50.679999589920044%\n",
      "Iteration: 198  Cost= 0.002821387698909697  Train accuracy= 87.85657051282051%  Test accuracy= 50.49999952316284%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter('data/logs/2')\n",
    "    summary_writer.add_graph(sess.graph)\n",
    "    # Training cycle\n",
    "    for iteration in range(epoch):\n",
    "        avg_cost = 0.\n",
    "        avg_train_accuracy = 0.\n",
    "        total_batch = int(images.shape[0] / batch_size)\n",
    "        # Loop over all batches\n",
    "        total_batch_time = 0.\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = images[i*batch_size:(i+1)*batch_size,:,:,:]\n",
    "            batch_ys = labels[i*batch_size:(i+1)*batch_size]\n",
    "            _, cost, train_accuracy, summary_str = sess.run([optimizer, loss, accuracy, summ], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += cost/batch_size/total_batch\n",
    "            avg_train_accuracy += train_accuracy/total_batch\n",
    "            summary_writer.add_summary(summary_str, iteration * total_batch + i)\n",
    "\n",
    "        if iteration % display_step == 0:\n",
    "            test_accuracy = sess.run([accuracy], feed_dict={x: test_images, y: test_labels})\n",
    "            print(\"Iteration: {}  Cost= {}  Train accuracy= {}%  Test accuracy= {}%\".format(iteration, avg_cost, avg_train_accuracy*100,test_accuracy[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
